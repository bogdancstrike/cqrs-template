\# Read Model Synchronization Strategy \- Alert Management System

This document details the strategy for synchronizing the read model (stored in Elasticsearch) with the write model (events in the Event Store managed by PostgreSQL) in the Alert Management System.

\#\# 1\. Overview

The system employs a CQRS architecture where the read model is updated asynchronously based on domain events generated by the command side. This leads to \*\*eventual consistency\*\*. The primary mechanism for this synchronization is Axon Framework's \*\*Event Processors\*\* (specifically, Tracking Event Processors).

\#\# 2\. Core Components Involved

\* \*\*Event Store (PostgreSQL):\*\* The single source of truth, storing all domain events.  
\* \*\*Event Bus (Axon, potentially via Kafka):\*\* Distributes domain events after they are persisted.  
\* \*\*Tracking Event Processor (TEP \- Axon):\*\* A type of event processor that actively pulls events from the event store (or a stream like Kafka if configured as the source for the TEP). TEPs maintain a "tracking token" to keep track of their progress.  
    \* \*\*Token Store (JPA/PostgreSQL):\*\* Stores the tracking tokens for TEPs, allowing them to resume from where they left off after a restart or failure.  
\* \*\*Alert Projector (\`AlertProjection\` class):\*\* An event handling component containing \`@EventHandler\` methods. This component is managed by a TEP. It consumes domain events (e.g., \`AlertCreatedEvent\`, \`AlertUpdatedEvent\`) and updates the Elasticsearch read model.  
\* \*\*Elasticsearch Client/Repository:\*\* Used by the \`AlertProjector\` to interact with the Elasticsearch index.  
\* \*\*Elasticsearch Index (\`alerts\`):\*\* The denormalized data store for alerts, optimized for querying.

\#\# 3\. Synchronization Flow

1\.  \*\*Event Generation & Persistence:\*\*  
    \* An \`AlertAggregate\` processes a command and emits one or more domain events.  
    \* Axon Framework persists these events atomically to the Event Store (PostgreSQL).  
2\.  \*\*Event Publication:\*\*  
    \* After successful persistence, Axon publishes the events to the configured Event Bus. If Kafka is used as the event bus for Axon, events are published to a specific Kafka topic (e.g., \`alerts-events-topic\`).  
3\.  \*\*Event Consumption by Tracking Event Processor (TEP):\*\*  
    \* The \`AlertProjector\` is associated with a Tracking Event Processor.  
    \* This TEP continuously polls the Event Bus (or the Event Store directly if Kafka is not the intermediate bus for TEPs) for new events since its last processed event (tracked by its token).  
4\.  \*\*Event Handling by Projector:\*\*  
    \* When the TEP fetches new events, it delivers them to the \`@EventHandler\` methods within the \`AlertProjector\`.  
    \* For example, an \`AlertCreatedEvent\` will trigger a method that creates a new alert document in Elasticsearch. An \`AlertAcknowledgedEvent\` will trigger a method that updates the status and \`acknowledgedAt\` field of an existing alert document.  
5\.  \*\*Read Model Update (Elasticsearch):\*\*  
    \* The projector uses an Elasticsearch client (e.g., via \`ElasticsearchRepository\`) to perform create, update, or delete operations on the \`alerts\` index.  
6\.  \*\*Tracking Token Update:\*\*  
    \* After successfully processing a batch of events (or a single event, depending on configuration), the TEP updates its tracking token in the Token Store (PostgreSQL) to reflect the new position in the event stream. This ensures that if the application restarts or the TEP fails, it can resume processing from the correct point, preventing data loss or duplicate processing (if handlers are idempotent).

\#\# 4\. Batching Logic for Elasticsearch Updates

To optimize performance and reduce the load on Elasticsearch, especially during high event throughput, updates to the read model are batched.

\* \*\*Requirement:\*\* Flush either every 100 messages (events) or after 2 minutes, whichever comes first.

\* \*\*Implementation Strategy:\*\*  
    1\.  \*\*Buffer:\*\* The \`AlertProjector\` will maintain an in-memory buffer (e.g., a \`List\` of \`WriteRequest\` objects for Elasticsearch's Bulk API).  
    2\.  \*\*Event Accumulation:\*\* As events arrive, the projector translates them into Elasticsearch \`IndexRequest\`, \`UpdateRequest\`, or \`DeleteRequest\` objects and adds them to this buffer.  
    3\.  \*\*Size-Based Flushing:\*\* After processing each event, the projector checks if the buffer size has reached the configured batch size (e.g., 100). If so, it triggers a flush.  
    4\.  \*\*Time-Based Flushing:\*\* A separate \`ScheduledExecutorService\` (or Spring's \`@Scheduled\` task) will run periodically (e.g., every few seconds). This task will check the time elapsed since the last flush or since the first event was added to the current batch. If the timeout (e.g., 2 minutes) is exceeded and the buffer is not empty, it will trigger a flush.  
    5\.  \*\*Flush Operation:\*\*  
        \* The flush operation involves sending the buffered requests to Elasticsearch using its \*\*Bulk API\*\*. This is much more efficient than sending individual requests.  
        \* After a successful bulk operation, the buffer is cleared.  
        \* If the bulk operation fails, appropriate error handling and retry logic must be implemented. The TEP's error handling capabilities can be leveraged here (e.g., retrying, skipping, or moving to a dead-letter queue if events are problematic).  
    6\.  \*\*Shutdown Handling:\*\* Ensure that on application shutdown, any pending events in the buffer are flushed to Elasticsearch. Axon's TEPs have shutdown lifecycle management that can be used for this.

    \*\*Configuration (via \`application.properties\`):\*\*  
    \`\`\`properties  
    app.projection.batch.size=100  
    app.projection.batch.timeout-ms=120000 \# 2 minutes (120,000 ms)  
    \# Potentially a scheduler interval for checking timeout  
    \# app.projection.batch.scheduler-interval-ms=5000 \# 5 seconds  
    \`\`\`

\#\# 5\. Eventual Consistency Considerations

\* \*\*Latency:\*\* There will be a delay between when a command is processed (and an event is stored) and when the read model in Elasticsearch reflects that change. This latency depends on:  
    \* Event bus performance (if used).  
    \* TEP polling interval.  
    \* Processing time of the projector.  
    \* Batching timeout.  
    \* Elasticsearch indexing speed.  
\* \*\*Client Expectations:\*\* Clients querying the API should be aware that they might be reading slightly stale data. For most alert management UIs, this is acceptable. If near real-time consistency is critical for specific operations, alternative strategies might be needed (though this usually complicates the system).

\#\# 6\. Error Handling and Resilience

\* \*\*Tracking Event Processors (TEPs):\*\* TEPs in Axon provide built-in error handling. If an \`@EventHandler\` method in the projector throws an exception:  
    \* The TEP can be configured to retry processing the event.  
    \* After a certain number of retries, it can skip the event or move it to a "dead letter queue" (DLQ) if one is configured.  
    \* Processing of subsequent events for that TEP will typically pause until the problematic event is handled or skipped.  
\* \*\*Idempotency:\*\* Projector logic should ideally be idempotent. If an event is processed more than once (e.g., due to a retry after a temporary Elasticsearch unavailability), it should not lead to incorrect data in the read model (e.g., creating duplicate documents). Using the event's unique ID or the aggregate ID as the Elasticsearch document ID can help achieve this for creates. Updates should be based on current state.  
\* \*\*Elasticsearch Unavailability:\*\* The projector should handle potential unavailability of Elasticsearch gracefully (e.g., with retries using libraries like Spring Retry, or by relying on TEP retries).

\#\# 7\. Replaying Events

One of the powerful features of Event Sourcing and Axon's TEPs is the ability to rebuild read models by replaying events from the Event Store.

\* \*\*Scenario:\*\* If the read model schema changes, a bug is found in the projection logic, or the read model becomes corrupted, it can be rebuilt.  
\* \*\*Process:\*\*  
    1\.  Delete the existing Elasticsearch index (or create a new one).  
    2\.  Reset the tracking token for the \`AlertProjector\`'s TEP in the Token Store.  
    3\.  Restart the application (or just the TEP). The TEP will start processing events from the beginning of the Event Store, rebuilding the read model in Elasticsearch.  
    \* This process can take time for large event stores, so it should be planned carefully.

By using Axon's Tracking Event Processors with appropriate batching and error handling, the Alert Management System can maintain a resilient and eventually consistent read model in Elasticsearch.  
