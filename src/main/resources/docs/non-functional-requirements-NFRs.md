# **Non-Functional Requirements (NFRs) \- Alert Management System**

This document outlines the key Non-Functional Requirements (NFRs) for the Alert Management System. These requirements define the quality attributes of the system, such as performance, scalability, reliability, and maintainability.

## **1\. Performance**

* **P1: Alert Ingestion Latency (Kafka to Command Processed):**  
  * **Target:** 95% of incoming Kafka messages (leading to CreateAlertCommand) should be processed by the command side (event persisted in Event Store) within **500 milliseconds** under normal load conditions.  
  * **Peak Load Target:** 95% within **1 second** under peak load.  
* **P2: Read Model Update Latency (Event Persisted to Read Model Updated):**  
  * **Target:** The time from an event being persisted in the Event Store to the corresponding update being visible in the Elasticsearch read model should be less than **5 seconds** for 95% of events under normal load (considering batching timeout of 2 minutes as a worst-case for a single event if no other events arrive).  
  * **With Batching:** The average latency will be influenced by batch size and timeout. The goal is for the system to keep up with the event stream without the TEP falling significantly behind.  
* **P3: API Query Response Time:**  
  * **Target:** 95% of standard API query requests (e.g., find by status, get all with pagination) should be served in under **200 milliseconds**.  
  * **Complex Searches:** 95% of complex keyword searches in Elasticsearch should be served in under **500 milliseconds**.  
* **P4: Throughput \- Command Side:**  
  * **Target:** The command side should be able to process at least **100 alert creation commands per second** sustained.  
* **P5: Throughput \- Query Side:**  
  * **Target:** The query side API should be able to handle at least **200 read queries per second** sustained.

## **2\. Scalability**

* **SC1: Horizontal Scalability (Command Side):** The command handling components (including Kafka consumers and Axon command handlers if distributed) should be scalable horizontally by adding more instances to handle increased load.  
* **SC2: Horizontal Scalability (Query Side):** The query handling components (REST controllers, query handlers) and Elasticsearch itself should be scalable horizontally.  
* **SC3: Event Processor Scalability:** Axon Tracking Event Processors can be configured for parallel processing by splitting the event stream using a segmentationKey (e.g., based on aggregateId or another consistent hash). This allows multiple threads or instances of a projector to work on different segments of the event stream concurrently.  
* **SC4: Database Scalability:**  
  * **PostgreSQL (Event Store):** Should support strategies like read replicas if parts of Axon (e.g., token store reads) become a bottleneck, though the primary event store writes are to the master. For very high volumes, partitioning might be considered in the long term.  
  * **Elasticsearch:** Natively supports horizontal scaling by adding more nodes to the cluster.

## **3\. Reliability & Availability**

* **R1: System Availability:**  
  * **Target:** The core alert processing pipeline (Kafka ingestion to event persistence) and query API should achieve **99.9% uptime**.  
* **R2: Data Durability (Event Store):** Events stored in PostgreSQL must be durable and protected against loss. Standard database backup and replication strategies are required.  
* **R3: Data Durability (Read Model):** Elasticsearch data should be replicated across multiple nodes to prevent data loss in case of a single node failure. Regular snapshots of Elasticsearch indices are recommended.  
* **R4: Fault Tolerance:**  
  * The system should be resilient to transient failures of its dependencies (PostgreSQL, Elasticsearch, Kafka). Components should implement retry mechanisms.  
  * Failure of one instance of a scaled component should not lead to system unavailability; traffic should be routed to healthy instances.  
  * Tracking Event Processors should automatically resume from their last known position after a restart or transient failure.  
* **R5: No Message Loss (Input):** Kafka input messages should not be lost. This requires proper Kafka consumer configuration (e.g., enable.auto.commit=false and manual offset commits after successful processing).  
* **R6: No Event Loss (Domain Events):** Domain events generated by aggregates must be reliably persisted and published. Axon Framework's mechanisms ensure this for the event store. If Kafka is used for event distribution, its own durability guarantees apply.

## **4\. Maintainability**

* **M1: Modularity:** The system should maintain a clear modular structure (as defined by the Maven modules) to facilitate independent development, testing, and deployment of components.  
* **M2: Code Quality:** Code should adhere to established coding standards, be well-documented, and include comprehensive comments.  
* **M3: Testability:** All components should be designed for testability. High unit and integration test coverage (\>80%) is required.  
* **M4: Configurability:** Key parameters (e.g., batch sizes, timeouts, Kafka topics, database connections) should be configurable externally (e.g., via application.properties or environment variables).  
* **M5: Monitoring & Logging:** The system must provide comprehensive logging for diagnostics and troubleshooting. Integration with monitoring tools (e.g., Prometheus, Grafana via Spring Boot Actuator metrics) is essential for observing system health and performance.  
* **M6: Ease of Deployment:** The application should be easily deployable using Docker containers, with clear instructions for setup and configuration.

## **5\. Security**

* **S1: API Security:** REST APIs exposed by the system must be secured (e.g., using OAuth2/OIDC, API keys, or other appropriate authentication and authorization mechanisms). This is not implemented in the initial template but is a critical NFR for production.  
* **S2: Data Security:** Sensitive data within alerts (if any) should be handled according to data protection policies. Consider encryption at rest and in transit for all data stores and communication channels.  
* **S3: Dependency Security:** Regularly scan and update dependencies to mitigate known vulnerabilities.  
* **S4: Access Control:** Ensure proper access controls for infrastructure components (PostgreSQL, Elasticsearch, Kafka).

## **6\. Usability (for Developers/Operators)**

* **U1: Documentation:** Comprehensive documentation (README, ADRs, business docs, API docs) must be provided and kept up-to-date.  
* **U2: Ease of Setup:** Local development setup should be straightforward using Docker Compose.  
* **U3: Debuggability:** The system should be designed to be easily debugged, with clear logging and traceability of requests/events.

## **7\. Batch Latency & Throughput Assumptions (for Read Model Sync)**

* **BL1: Batch Size:** The Elasticsearch projection batch size is configured to 100 events.  
* **BL2: Batch Timeout:** The Elasticsearch projection batch timeout is configured to 2 minutes (120,000 ms).  
* **BL3: Assumed Event Rate for Batching:** The batching strategy assumes that under normal load, event rates will be high enough to frequently fill the batch size, leading to flushes before the 2-minute timeout. The timeout primarily acts as a safeguard for periods of low event activity to ensure pending events are eventually processed.  
* **BL4: Throughput Assumption:** The read model synchronization (projectors) must be able to keep up with the sustained throughput of the command side (e.g., 100 events/sec). If a single projector instance cannot keep up, TEP segmentation will be necessary.

These NFRs will guide the design, development, and operational considerations for the Alert Management System. They should be reviewed and potentially refined as the project progresses.